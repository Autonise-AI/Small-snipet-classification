{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterances</th>\n",
       "      <th>Intents</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>POSextracts(NN,VBG) [not included in data ]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how to access google</td>\n",
       "      <td>offer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>access google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how to access missed call alert</td>\n",
       "      <td>ideafiber_usage_alert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>access call alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how to access my sd card on myideafi device</td>\n",
       "      <td>ideacinema_download_externalmemory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>access card myjiofi device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how to access myidea fi device for changing pa...</td>\n",
       "      <td>use_ideafi_device</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>access myjio fi device changing password</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how to access that one video is taking how muc...</td>\n",
       "      <td>ideaNewsPaper_devices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>access video taking gb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Utterances  \\\n",
       "0                               how to access google   \n",
       "1                    how to access missed call alert   \n",
       "2        how to access my sd card on myideafi device   \n",
       "3  how to access myidea fi device for changing pa...   \n",
       "4  how to access that one video is taking how muc...   \n",
       "\n",
       "                              Intents  Unnamed: 2  Unnamed: 3  Unnamed: 4  \\\n",
       "0                               offer         NaN         NaN         NaN   \n",
       "1               ideafiber_usage_alert         NaN         NaN         NaN   \n",
       "2  ideacinema_download_externalmemory         NaN         NaN         NaN   \n",
       "3                   use_ideafi_device         NaN         NaN         NaN   \n",
       "4               ideaNewsPaper_devices         NaN         NaN         NaN   \n",
       "\n",
       "   Unnamed: 5  Unnamed: 6  Unnamed: 7  \\\n",
       "0         NaN         NaN         NaN   \n",
       "1         NaN         NaN         NaN   \n",
       "2         NaN         NaN         NaN   \n",
       "3         NaN         NaN         NaN   \n",
       "4         NaN         NaN         NaN   \n",
       "\n",
       "  POSextracts(NN,VBG) [not included in data ]  \n",
       "0                               access google  \n",
       "1                           access call alert  \n",
       "2                  access card myjiofi device  \n",
       "3    access myjio fi device changing password  \n",
       "4                      access video taking gb  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('dataNLU.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['Utterances']\n",
    "data_1 = data[0:4000]\n",
    "test_data = data[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/krishna/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprcessing\n",
    "# Stemming the words and splitting each sentence into relevant tokens\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token)>3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['how', 'to', 'use', 'data', 'booster']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['data', 'booster']\n"
     ]
    }
   ],
   "source": [
    "# Verify preprocessing\n",
    "\n",
    "doc_sample = data[4000]\n",
    "print('original document: ')\n",
    "print(doc_sample.split())\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             [access, googl]\n",
       "1                       [access, miss, alert]\n",
       "2             [access, card, myideafi, devic]\n",
       "3    [access, myidea, devic, chang, password]\n",
       "4                       [access, video, take]\n",
       "5                              [account, set]\n",
       "6                                   [account]\n",
       "7                                   [account]\n",
       "8                             [account, idea]\n",
       "9               [achiev, idea, celebr, offer]\n",
       "Name: Utterances, dtype: object"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem and tokenize each training example\n",
    "\n",
    "processed_data = data_1.map(preprocess)\n",
    "processed_test = test_data.map(preprocess)\n",
    "processed_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17, 1)]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary from processed_data containing number of times a word appears in training set\n",
    "dictionary = gensim.corpora.Dictionary(processed_data)\n",
    "test_dict = gensim.corpora.Dictionary(processed_test)\n",
    "\n",
    "# Filter out tokens \n",
    "# Keep only tokens that occur more than 3 times and less than in 85 percent of the dataset\n",
    "# Keep only the 4000 most frequent tokens\n",
    "dictionary.filter_extremes(no_below = 3, no_above = 0.85, keep_n = 4000)\n",
    "test_dict.filter_extremes(no_below = 3, no_above = 0.95, keep_n = 4000)\n",
    "\n",
    "#For each data, create a dictionary to store how many words and how many times does a word appear in a sentence\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_data]\n",
    "test_corpus = [test_dict.doc2bow(doc) for doc in processed_test]\n",
    "\n",
    "bow_corpus[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 17 (\"data\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview bag of words for our sample preprocessed document\n",
    "\n",
    "bow_doc_sample = bow_corpus[-1]\n",
    "\n",
    "for i in range(len(bow_doc_sample)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_sample[i][0], \n",
    "                                               dictionary[bow_doc_sample[i][0]], \n",
    "bow_doc_sample[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning using LDA bag of words and TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7272418883021802), (1, 0.6863812613254235)]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Model\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " Words: 0.165*\"idea\" + 0.094*\"prime\" + 0.069*\"stop\" + 0.068*\"tone\" + 0.055*\"membership\" + 0.053*\"servic\" + 0.039*\"appli\" + 0.039*\"alert\" + 0.037*\"miss\" + 0.035*\"logout\"\n",
      "Topic: 1 \n",
      " Words: 0.190*\"offer\" + 0.157*\"voucher\" + 0.146*\"redeem\" + 0.126*\"idea\" + 0.044*\"paytm\" + 0.044*\"unlock\" + 0.034*\"code\" + 0.028*\"coupon\" + 0.027*\"rupe\" + 0.024*\"celebr\"\n",
      "Topic: 2 \n",
      " Words: 0.289*\"chang\" + 0.192*\"plan\" + 0.068*\"idea\" + 0.065*\"password\" + 0.057*\"messag\" + 0.038*\"avail\" + 0.021*\"usernam\" + 0.018*\"current\" + 0.018*\"reach\" + 0.018*\"loan\"\n",
      "Topic: 3 \n",
      " Words: 0.290*\"activ\" + 0.158*\"pack\" + 0.105*\"ideasim\" + 0.101*\"idea\" + 0.101*\"celebr\" + 0.083*\"ideaphon\" + 0.014*\"gvoic\" + 0.014*\"purchas\" + 0.011*\"ring\" + 0.010*\"exchang\"\n",
      "Topic: 4 \n",
      " Words: 0.416*\"data\" + 0.156*\"free\" + 0.063*\"idea\" + 0.049*\"ideaapp\" + 0.040*\"money\" + 0.026*\"share\" + 0.020*\"booster\" + 0.020*\"cashback\" + 0.017*\"extra\" + 0.017*\"earn\"\n",
      "Topic: 5 \n",
      " Words: 0.163*\"milk\" + 0.148*\"dairi\" + 0.132*\"call\" + 0.078*\"cadburi\" + 0.060*\"data\" + 0.056*\"send\" + 0.040*\"photo\" + 0.039*\"receiv\" + 0.035*\"music\" + 0.030*\"ideamoney\"\n",
      "Topic: 6 \n",
      " Words: 0.186*\"account\" + 0.146*\"myidea\" + 0.089*\"remov\" + 0.074*\"tune\" + 0.067*\"ideanumb\" + 0.054*\"caller\" + 0.049*\"idea\" + 0.046*\"creat\" + 0.032*\"sign\" + 0.031*\"book\"\n",
      "Topic: 7 \n",
      " Words: 0.133*\"network\" + 0.074*\"updat\" + 0.074*\"connect\" + 0.052*\"youtub\" + 0.048*\"video\" + 0.047*\"song\" + 0.040*\"solv\" + 0.036*\"track\" + 0.033*\"improv\" + 0.032*\"setup\"\n",
      "Topic: 8 \n",
      " Words: 0.124*\"delet\" + 0.105*\"deactiv\" + 0.078*\"voic\" + 0.076*\"myideanumb\" + 0.059*\"block\" + 0.051*\"number\" + 0.050*\"histori\" + 0.041*\"ideafi\" + 0.041*\"idea\" + 0.039*\"set\"\n",
      "Topic: 9 \n",
      " Words: 0.127*\"speed\" + 0.121*\"increas\" + 0.104*\"play\" + 0.089*\"myideaapp\" + 0.060*\"ideamus\" + 0.057*\"idea\" + 0.049*\"login\" + 0.026*\"ideanet\" + 0.023*\"cancel\" + 0.021*\"valid\"\n",
      "Topic: 10 \n",
      " Words: 0.442*\"recharg\" + 0.178*\"phone\" + 0.083*\"number\" + 0.059*\"mobil\" + 0.019*\"bank\" + 0.018*\"watch\" + 0.018*\"ideaphon\" + 0.017*\"idea\" + 0.015*\"order\" + 0.012*\"listen\"\n",
      "Topic: 11 \n",
      " Words: 0.138*\"number\" + 0.136*\"know\" + 0.091*\"download\" + 0.052*\"balanc\" + 0.040*\"onlin\" + 0.034*\"transfer\" + 0.029*\"hack\" + 0.029*\"roam\" + 0.027*\"close\" + 0.027*\"enabl\"\n",
      "Topic: 12 \n",
      " Words: 0.314*\"check\" + 0.115*\"mobil\" + 0.104*\"internet\" + 0.085*\"data\" + 0.029*\"extra\" + 0.025*\"reset\" + 0.024*\"lose\" + 0.023*\"balanc\" + 0.021*\"ideaid\" + 0.017*\"collect\"\n",
      "Topic: 13 \n",
      " Words: 0.085*\"regist\" + 0.082*\"card\" + 0.074*\"open\" + 0.065*\"idea\" + 0.062*\"ideatv\" + 0.052*\"devic\" + 0.044*\"switch\" + 0.042*\"save\" + 0.038*\"start\" + 0.030*\"ideasim\"\n",
      "Topic: 14 \n",
      " Words: 0.334*\"idea\" + 0.090*\"number\" + 0.068*\"mobil\" + 0.066*\"tune\" + 0.063*\"rington\" + 0.051*\"connect\" + 0.040*\"link\" + 0.037*\"coupon\" + 0.030*\"chang\" + 0.028*\"instal\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=15, id2word=dictionary, passes=25, workers=2)\n",
    "lda_model_topics = []\n",
    "\n",
    "# Printing the different topics recognised by bag of words model\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\n Words: {}'.format(idx, topic))\n",
    "    lda_model_topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " Word: 0.137*\"know\" + 0.076*\"call\" + 0.065*\"unlock\" + 0.063*\"myideanumb\" + 0.054*\"regist\" + 0.053*\"money\" + 0.050*\"appli\" + 0.041*\"ideamus\" + 0.038*\"set\" + 0.032*\"save\"\n",
      "Topic: 1 \n",
      " Word: 0.243*\"recharg\" + 0.090*\"remov\" + 0.079*\"number\" + 0.073*\"delet\" + 0.045*\"phone\" + 0.044*\"card\" + 0.036*\"tone\" + 0.035*\"histori\" + 0.033*\"idea\" + 0.032*\"cancel\"\n",
      "Topic: 2 \n",
      " Word: 0.137*\"tune\" + 0.097*\"play\" + 0.089*\"download\" + 0.065*\"caller\" + 0.050*\"idea\" + 0.046*\"video\" + 0.045*\"instal\" + 0.026*\"track\" + 0.025*\"custom\" + 0.022*\"whatsapp\"\n",
      "Topic: 3 \n",
      " Word: 0.134*\"mobil\" + 0.082*\"ideanumb\" + 0.079*\"milk\" + 0.074*\"dairi\" + 0.055*\"creat\" + 0.045*\"recharg\" + 0.043*\"cadburi\" + 0.041*\"link\" + 0.038*\"data\" + 0.030*\"booster\"\n",
      "Topic: 4 \n",
      " Word: 0.157*\"account\" + 0.089*\"ideaapp\" + 0.060*\"sign\" + 0.053*\"open\" + 0.051*\"network\" + 0.048*\"devic\" + 0.048*\"logout\" + 0.032*\"cashback\" + 0.028*\"roam\" + 0.027*\"setup\"\n",
      "Topic: 5 \n",
      " Word: 0.207*\"data\" + 0.142*\"activ\" + 0.109*\"plan\" + 0.097*\"ideasim\" + 0.061*\"phone\" + 0.056*\"voic\" + 0.050*\"ideatv\" + 0.042*\"transfer\" + 0.041*\"check\" + 0.037*\"idea\"\n",
      "Topic: 6 \n",
      " Word: 0.167*\"idea\" + 0.098*\"deactiv\" + 0.096*\"internet\" + 0.068*\"password\" + 0.033*\"record\" + 0.032*\"ideatun\" + 0.032*\"bank\" + 0.032*\"chang\" + 0.030*\"reset\" + 0.026*\"port\"\n",
      "Topic: 7 \n",
      " Word: 0.088*\"messag\" + 0.086*\"updat\" + 0.064*\"youtub\" + 0.063*\"onlin\" + 0.051*\"wifi\" + 0.048*\"prime\" + 0.045*\"idea\" + 0.038*\"earn\" + 0.038*\"user\" + 0.025*\"connect\"\n",
      "Topic: 8 \n",
      " Word: 0.232*\"free\" + 0.107*\"connect\" + 0.100*\"myidea\" + 0.068*\"data\" + 0.050*\"idea\" + 0.038*\"start\" + 0.031*\"ideafi\" + 0.030*\"music\" + 0.027*\"photo\" + 0.022*\"order\"\n",
      "Topic: 9 \n",
      " Word: 0.120*\"increas\" + 0.111*\"speed\" + 0.062*\"balanc\" + 0.061*\"extra\" + 0.045*\"switch\" + 0.043*\"data\" + 0.038*\"improv\" + 0.031*\"hello\" + 0.030*\"boost\" + 0.029*\"ideanet\"\n",
      "Topic: 10 \n",
      " Word: 0.139*\"rington\" + 0.103*\"send\" + 0.087*\"block\" + 0.051*\"rupe\" + 0.039*\"loan\" + 0.035*\"purchas\" + 0.032*\"idea\" + 0.032*\"ideacar\" + 0.031*\"ideanetwork\" + 0.026*\"join\"\n",
      "Topic: 11 \n",
      " Word: 0.203*\"voucher\" + 0.186*\"redeem\" + 0.088*\"login\" + 0.065*\"stop\" + 0.049*\"paytm\" + 0.039*\"recharg\" + 0.031*\"myideasim\" + 0.030*\"coupon\" + 0.023*\"current\" + 0.022*\"myidea\"\n",
      "Topic: 12 \n",
      " Word: 0.152*\"pack\" + 0.149*\"offer\" + 0.126*\"celebr\" + 0.069*\"idea\" + 0.064*\"activ\" + 0.044*\"avail\" + 0.034*\"data\" + 0.030*\"hack\" + 0.027*\"claim\" + 0.025*\"today\"\n",
      "Topic: 13 \n",
      " Word: 0.149*\"ideaphon\" + 0.130*\"check\" + 0.092*\"myideaapp\" + 0.064*\"coupon\" + 0.046*\"servic\" + 0.046*\"book\" + 0.039*\"recharg\" + 0.031*\"alert\" + 0.029*\"song\" + 0.029*\"miss\"\n",
      "Topic: 14 \n",
      " Word: 0.255*\"chang\" + 0.125*\"number\" + 0.054*\"share\" + 0.053*\"membership\" + 0.043*\"contact\" + 0.042*\"idea\" + 0.030*\"data\" + 0.029*\"plan\" + 0.027*\"reach\" + 0.020*\"prepay\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=15, id2word=dictionary, passes=25, workers=4)\n",
    "lda_model_tfidf_topics = []\n",
    "\n",
    "# Printing the different topics recognised by tf-idf model\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\n Word: {}'.format(idx, topic))\n",
    "    lda_model_tfidf_topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable_to_download_app\n",
      "how to increase idea downloading speed\n",
      "['increas', 'idea', 'download', 'speed']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Intents\"][2000])\n",
    "print(data_1[2000])\n",
    "print(processed_data[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.45875915819628926\t \n",
      "Topic: 0.120*\"increas\" + 0.111*\"speed\" + 0.062*\"balanc\" + 0.061*\"extra\" + 0.045*\"switch\"\n",
      "\n",
      "Score: 0.3679072623051828\t \n",
      "Topic: 0.137*\"tune\" + 0.097*\"play\" + 0.089*\"download\" + 0.065*\"caller\" + 0.050*\"idea\"\n",
      "\n",
      "Score: 0.013333406128404108\t \n",
      "Topic: 0.167*\"idea\" + 0.098*\"deactiv\" + 0.096*\"internet\" + 0.068*\"password\" + 0.033*\"record\"\n",
      "\n",
      "Score: 0.013333363417403968\t \n",
      "Topic: 0.152*\"pack\" + 0.149*\"offer\" + 0.126*\"celebr\" + 0.069*\"idea\" + 0.064*\"activ\"\n",
      "\n",
      "Score: 0.013333354848762843\t \n",
      "Topic: 0.232*\"free\" + 0.107*\"connect\" + 0.100*\"myidea\" + 0.068*\"data\" + 0.050*\"idea\"\n",
      "\n",
      "Score: 0.01333335254714329\t \n",
      "Topic: 0.088*\"messag\" + 0.086*\"updat\" + 0.064*\"youtub\" + 0.063*\"onlin\" + 0.051*\"wifi\"\n",
      "\n",
      "Score: 0.013333351098967895\t \n",
      "Topic: 0.255*\"chang\" + 0.125*\"number\" + 0.054*\"share\" + 0.053*\"membership\" + 0.043*\"contact\"\n",
      "\n",
      "Score: 0.01333334926875857\t \n",
      "Topic: 0.207*\"data\" + 0.142*\"activ\" + 0.109*\"plan\" + 0.097*\"ideasim\" + 0.061*\"phone\"\n",
      "\n",
      "Score: 0.013333347470686041\t \n",
      "Topic: 0.243*\"recharg\" + 0.090*\"remov\" + 0.079*\"number\" + 0.073*\"delet\" + 0.045*\"phone\"\n",
      "\n",
      "Score: 0.013333346643817119\t \n",
      "Topic: 0.139*\"rington\" + 0.103*\"send\" + 0.087*\"block\" + 0.051*\"rupe\" + 0.039*\"loan\"\n",
      "\n",
      "Score: 0.013333344378783007\t \n",
      "Topic: 0.157*\"account\" + 0.089*\"ideaapp\" + 0.060*\"sign\" + 0.053*\"open\" + 0.051*\"network\"\n",
      "\n",
      "Score: 0.0133333438760932\t \n",
      "Topic: 0.137*\"know\" + 0.076*\"call\" + 0.065*\"unlock\" + 0.063*\"myideanumb\" + 0.054*\"regist\"\n",
      "\n",
      "Score: 0.01333334095593959\t \n",
      "Topic: 0.149*\"ideaphon\" + 0.130*\"check\" + 0.092*\"myideaapp\" + 0.064*\"coupon\" + 0.046*\"servic\"\n",
      "\n",
      "Score: 0.013333340087156368\t \n",
      "Topic: 0.134*\"mobil\" + 0.082*\"ideanumb\" + 0.079*\"milk\" + 0.074*\"dairi\" + 0.055*\"creat\"\n",
      "\n",
      "Score: 0.013333338776611798\t \n",
      "Topic: 0.203*\"voucher\" + 0.186*\"redeem\" + 0.088*\"login\" + 0.065*\"stop\" + 0.049*\"paytm\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[2000]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics_data = []\n",
    "lda_tfidf_data_topics = []\n",
    "\n",
    "# test_data = data[4000:]\n",
    "\n",
    "for i in np.arange(len(test_data)):\n",
    "    sentence = test_data[i+4000]\n",
    "    lda_index = sorted(lda_model[test_corpus[i]], key=lambda tup: -1*tup[1])[0][0]\n",
    "    tfidf_index = sorted(lda_model_tfidf[test_corpus[i]], key=lambda tup: -1*tup[1])[0][0]\n",
    "    lda_topics_data.append(lda_model.print_topic(lda_index, 4))\n",
    "    lda_tfidf_data_topics.append(lda_model_tfidf.print_topic(tfidf_index, 4))\n",
    "#     print(lda_index, tfidf_index)\n",
    "#     print(\"Sentence:\", sentence)\n",
    "#     print(\"LDA Model Topic:\",  lda_model.print_topic(topic_index, 4))\n",
    "#     print(\"LDA-TFIDF Model Topic:\",  lda_model_tfidf.print_topic(topic_index, 4))\n",
    "#     print('\\n')\n",
    "    \n",
    "\n",
    "dataframe = {\n",
    "    \"Data\" : test_data, \n",
    "    \"LDA_topic\" : lda_topics_data,\n",
    "    \"LDA_topic_tfidf\" : lda_tfidf_data_topics\n",
    "}\n",
    "\n",
    "labelled_dataframe = pd.DataFrame(dataframe)\n",
    "labelled_dataframe.to_csv(\"LDA_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modelling works well for a lot of examples topics that are represented well in the dataset. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data                                  how to activate 10gb data free\n",
      "LDA_topic          0.475*\"data\" + 0.158*\"free\" + 0.120*\"chang\" + ...\n",
      "LDA_topic_tfidf    0.260*\"data\" + 0.180*\"free\" + 0.054*\"regist\" +...\n",
      "Name: 12, dtype: object \n",
      "\n",
      "Data                                 how to activate 16 gb free data\n",
      "LDA_topic          0.475*\"data\" + 0.158*\"free\" + 0.120*\"chang\" + ...\n",
      "LDA_topic_tfidf    0.260*\"data\" + 0.180*\"free\" + 0.054*\"regist\" +...\n",
      "Name: 13, dtype: object \n",
      "\n",
      "Data                    how to activate 1gp cadbury dairy milk offer\n",
      "LDA_topic          0.259*\"plan\" + 0.108*\"milk\" + 0.102*\"dairi\" + ...\n",
      "LDA_topic_tfidf    0.166*\"check\" + 0.082*\"milk\" + 0.076*\"dairi\" +...\n",
      "Name: 14, dtype: object \n",
      "\n",
      "Data                  how to activate 2 years celebrate of idea pack\n",
      "LDA_topic          0.211*\"pack\" + 0.133*\"idea\" + 0.122*\"celebr\" +...\n",
      "LDA_topic_tfidf    0.161*\"idea\" + 0.124*\"pack\" + 0.118*\"plan\" + 0...\n",
      "Name: 15, dtype: object \n",
      "\n",
      "Data                                   how to activate 2gb data free\n",
      "LDA_topic          0.475*\"data\" + 0.158*\"free\" + 0.120*\"chang\" + ...\n",
      "LDA_topic_tfidf    0.260*\"data\" + 0.180*\"free\" + 0.054*\"regist\" +...\n",
      "Name: 16, dtype: object \n",
      "\n",
      "Data                                how to activate 2gb data per day\n",
      "LDA_topic          0.160*\"account\" + 0.159*\"ideaphon\" + 0.130*\"ac...\n",
      "LDA_topic_tfidf    0.150*\"activ\" + 0.139*\"ideaphon\" + 0.087*\"inte...\n",
      "Name: 17, dtype: object \n",
      "\n",
      "Data                                        how to activate 3gb data\n",
      "LDA_topic          0.160*\"account\" + 0.159*\"ideaphon\" + 0.130*\"ac...\n",
      "LDA_topic_tfidf    0.150*\"activ\" + 0.139*\"ideaphon\" + 0.087*\"inte...\n",
      "Name: 18, dtype: object \n",
      "\n",
      "Data                                        how to activate a remedy\n",
      "LDA_topic          0.160*\"account\" + 0.159*\"ideaphon\" + 0.130*\"ac...\n",
      "LDA_topic_tfidf    0.150*\"activ\" + 0.139*\"ideaphon\" + 0.087*\"inte...\n",
      "Name: 19, dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(12,18):\n",
    "    print(labelled_dataframe.iloc[i], \"\\n\")\n",
    "# print(labelled_dataframe.loc[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are a lot of examples similar to these and LDA does a reasonably good job on these examples. In the above examples, we can clearly see that the topic words directly relate to the data example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA struggles to assign topics to examples which are not represented very well in the dataset. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data                                            how to access google\n",
      "LDA_topic          0.160*\"account\" + 0.159*\"ideaphon\" + 0.130*\"ac...\n",
      "LDA_topic_tfidf    0.086*\"voic\" + 0.071*\"sign\" + 0.071*\"password\"...\n",
      "Name: 0, dtype: object \n",
      "\n",
      "Data                                 how to access missed call alert\n",
      "LDA_topic          0.259*\"plan\" + 0.108*\"milk\" + 0.102*\"dairi\" + ...\n",
      "LDA_topic_tfidf    0.089*\"transfer\" + 0.077*\"link\" + 0.068*\"stop\"...\n",
      "Name: 1, dtype: object \n",
      "\n",
      "Data                                            how to change gender\n",
      "LDA_topic          0.475*\"data\" + 0.158*\"free\" + 0.120*\"chang\" + ...\n",
      "LDA_topic_tfidf    0.203*\"chang\" + 0.076*\"login\" + 0.059*\"ideamus...\n",
      "Name: 528, dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [0,1,528]:\n",
    "    print(labelled_dataframe.iloc[i], \"\\n\")\n",
    "# print(labelled_dataframe.loc[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key tokens in the above examples are very specific to those examples and not found in other examples of the data. Hence, LDA does not properly classify these examples. Also expectedly, LDA does not properly classfiy examples that were labelled garbage in the original datas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
